# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/samuelajala01/house_price_prediction_model/blob/master/model.ipynb

House Price prediction Model using Python
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

data =pd.read_csv("https://raw.githubusercontent.com/samuelajala01/house_price_prediction_model/master/housing.csv")
data

data.info()

""".info is used to check if rows have null or empty values

.dropna is used to remove rows with null entries in the data
while inplace is used to replace previous dataset with this new data without re-assigning
"""

data.dropna(inplace=True)
data

"""Now we want to split the data into training and test data.
X is the dataframe without the target variable, so it will be removed, .drop is used to remove a feature, here it is the 'median_house_value' which should be our y.

y = data['median_house_value'] or y = data.median_house_value is valid
"""

from sklearn.model_selection import train_test_split

X = data.drop(['median_house_value'], axis=1)
y = data['median_house_value']

"""Now the splitting"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

train_data = X_train.join(y_train)

train_data.hist(figsize=(15, 8))

plt.figure(figsize=(15,8))
# sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu")

"""Because some data are skewed, we add log to them to make them more Gaussian"""

train_data['total_rooms'] = np.log(train_data['total_rooms'] + 1)
train_data['total_bedrooms'] = np.log(train_data['total_bedrooms'] + 1)
train_data['population'] = np.log(train_data['population'] + 1)
train_data['households'] = np.log(train_data['households'] + 1)

train_data.hist(figsize=(15, 8))

"""pd.get_dummies is used for one-hot encoding, here we're trying to encode ocean proximity, we then add the encoded feature to the dataset using .join, then remove ocean proximity using .drop"""

train_data = train_data.join(pd.get_dummies(train_data.ocean_proximity, dtype=int)).drop(['ocean_proximity'], axis =1)
train_data

plt.figure(figsize=(15, 8))

sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu")

plt.figure(figsize=(15, 8))
sns.scatterplot(x='latitude', y='longitude', data=train_data, hue="median_house_value", palette="coolwarm")

train_data["bedroom_ratio"] = train_data["total_bedrooms"] / train_data["total_rooms"]
train_data["households_rooms"] = train_data["total_rooms"] / train_data["households"]

plt.figure(figsize=(15,8))
sns.heatmap(train_data.corr(), annot=True, cmap="YlGnBu")

"""# Using Linear regression.

we splitted the data earlier than we should, but we'll have to just re-specify training, and test again
"""

from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()

X_train, y_train = train_data.drop(["median_house_value"], axis=1), train_data["median_house_value"]
X_train_s = scaler.fit_transform(X_train)

reg = LinearRegression()

reg.fit(X_train_s, y_train)

test_data = X_test.join(y_test)

test_data['total_rooms'] = np.log(test_data['total_rooms'] + 1)
test_data['total_bedrooms'] = np.log(test_data['total_bedrooms'] + 1)
test_data['population'] = np.log(test_data['population'] + 1)
test_data['households'] = np.log(test_data['households'] + 1)

test_data = test_data.join(pd.get_dummies(test_data.ocean_proximity, dtype=int)).drop(['ocean_proximity'], axis =1)

test_data["bedroom_ratio"] = test_data["total_bedrooms"] / test_data["total_rooms"]
test_data["households_rooms"] = test_data["total_rooms"] / test_data["households"]

test_data

X_test, y_test = test_data.drop(["median_house_value"], axis=1), test_data["median_house_value"]

"""To get the model's accuracy"""

X_test_s = scaler.transform(X_test)

print(reg.score(X_test_s, y_test))

"""Now Let's use L1 regualrization"""

from sklearn.linear_model import Lasso
lasso = Lasso(alpha=0.01).fit(X_train, y_train)

print(lasso.score(X_train, y_train))
print(lasso.score(X_test, y_test))

from sklearn.linear_model import Ridge
ridge = Ridge(alpha=0.7).fit(X_train, y_train)
print(f"Ridge Regression-Training set score: {ridge.score(X_train, y_train):.2f}")
print(f"Ridge Regression-Test set score: {ridge.score(X_test, y_test):.2f}")

from sklearn.ensemble import RandomForestRegressor

forest = RandomForestRegressor()
forest.fit(X_train_s, y_train)

forest.score(X_test_s, y_test)

from sklearn.model_selection import GridSearchCV

forest = RandomForestRegressor()

param_grid = {
    "n_estimators": [30,50,100],
    "max_features": [8,12,20],
    "min_samples_split": [2,4,6,8]
}

grid_search = GridSearchCV(forest, param_grid,cv=5,scoring="neg_mean_squared_error",return_train_score=True)

grid_search.fit(X_train_s, y_train)

best_forest = grid_search.best_estimator_

best_forest.score(X_test_s, y_test)